# Image-to-Image Translation with Pix2Pix

This project implements the Pix2Pix model, a conditional generative adversarial network (cGAN), for image-to-image translation. The goal is to learn a mapping from an input image to an output image. This implementation uses the facades dataset, which consists of pairs of images: a real building facade and its corresponding architectural label.

## 1. Data Loading and Preprocessing

The code begins by downloading and extracting the chosen dataset (in this case, "facades"). The dataset is then loaded into a `tf.data.Dataset` pipeline.

The following preprocessing steps are applied to the data:
* **Image Splitting:** Each image in the dataset is split into two separate images: the input image (architectural labels) and the real image (building facade).
* **Resizing and Cropping:** Images are resized to 286x286 and then randomly cropped to 256x256. This is a form of data augmentation that helps the model learn to be robust to small translations.
* **Random Jittering:**  Random jittering is applied to the training images, which includes random mirroring. This further increases the diversity of the training data.
* **Normalization:** The pixel values of the images are normalized to be in the range of [-1, 1].

## 2. The Generator Model

The generator is a U-Net based architecture. A U-Net is an encoder-decoder model with skip connections between the encoder and decoder layers. These skip connections allow the generator to share low-level information between the input and output, which is crucial for tasks like image-to-image translation where the output is structurally similar to the input.

The generator's role is to take an input image (the architectural labels) and generate a realistic-looking output image (the building facade).

## 3. The Discriminator Model

The discriminator is a PatchGAN classifier. Instead of classifying the entire image as real or fake, a PatchGAN classifies NxN patches of the image as real or fake. This encourages the generator to produce images that are locally realistic.

The discriminator's role is to distinguish between real images (from the dataset) and fake images (generated by the generator).

## 4. Loss Functions

The model uses two loss functions:
* **Generator Loss:** This is a combination of two losses:
    * **GAN Loss:** This measures how well the generator is able to fool the discriminator. It is calculated as the binary cross-entropy between the discriminator's predictions on the generated images and a tensor of all ones.
    * **L1 Loss:** This is the mean absolute error between the generated image and the target image. It encourages the generator to produce images that are structurally similar to the target images.
* **Discriminator Loss:** This measures how well the discriminator is able to distinguish between real and fake images. It is the sum of two losses:
    * The binary cross-entropy between the discriminator's predictions on the real images and a tensor of all ones.
    * The binary cross-entropy between the discriminator's predictions on the fake images and a tensor of all zeros.

## 5. Training

The model is trained by alternating between training the generator and the discriminator.
* **Generator Training:** The generator is trained to minimize the generator loss. This is done by feeding the generator an input image, generating an output image, and then using the discriminator to classify the generated image. The gradients of the generator loss with respect to the generator's weights are then calculated and used to update the generator's weights.
* **Discriminator Training:** The discriminator is trained to minimize the discriminator loss. This is done by feeding the discriminator both real and fake images and then updating the discriminator's weights based on how well it is able to classify them.

The training process is monitored using TensorBoard, which visualizes the loss functions over time. Checkpoints are saved periodically so that the training can be resumed later.
## 6. Results

After training, the generator is able to produce realistic-looking building facades from architectural labels. The `generate_images` function can be used to visualize the results on the test set.
